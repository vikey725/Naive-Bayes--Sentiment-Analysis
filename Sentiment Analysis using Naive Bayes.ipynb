{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "occupational-lodge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/vikash/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/vikash/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import twitter_samples \n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beautiful-philosophy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All positive tweets length:  5000\n",
      "All negative tweets length:  5000\n"
     ]
    }
   ],
   "source": [
    "positive_tweet_list = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweet_list = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "print(\"All positive tweets length: \", len(positive_tweet_list))\n",
    "print(\"All negative tweets length: \", len(negative_tweet_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "figured-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess tweet\n",
    "stemmer = PorterStemmer()\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "stopwords_eng = stopwords.words(\"english\")\n",
    "punctuations_eng = string.punctuation\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r\"\\$\\w*\", \"\", tweet)\n",
    "    \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r\"^RT[\\s]+\", \"\", tweet)\n",
    "    \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r\"https?:\\/\\/[.a-zA-Z\\/-]*[\\r\\n]*\", \"\", tweet)\n",
    "    \n",
    "    # remove the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    tweet_tokens_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_eng and word not in punctuations_eng):\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            \n",
    "            tweet_tokens_clean.append(stemmed_word)\n",
    "    \n",
    "    return tweet_tokens_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "gothic-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word frequencies\n",
    "\n",
    "def generate_words_frequency(tweet_list, vocab):\n",
    "    frequency_dict = {}\n",
    "    for tweet in tweet_list:\n",
    "        tweet_tokens = preprocess_tweet(tweet)\n",
    "        for word in tweet_tokens:\n",
    "            if word not in frequency_dict:\n",
    "                frequency_dict[word] = 1\n",
    "            else:\n",
    "                frequency_dict[word] += 1\n",
    "                \n",
    "    for word in vocab:\n",
    "        if word not in frequency_dict:\n",
    "            frequency_dict[word] = 0\n",
    "                \n",
    "    return frequency_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "architectural-roommate",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos, test_pos = positive_tweet_list[:4000], positive_tweet_list[4000:]\n",
    "train_neg, test_neg = negative_tweet_list[:4000], negative_tweet_list[4000:]\n",
    "\n",
    "vocab = set()\n",
    "train_x = train_pos + train_neg\n",
    "for tweet in train_x:\n",
    "    tweet_tokens = preprocess_tweet(tweet)\n",
    "    for word in tweet_tokens:\n",
    "        if word not in vocab:\n",
    "            vocab.add(word)\n",
    "            \n",
    "vocab = list(vocab)\n",
    "pos_frequency_dict = generate_words_frequency(train_pos, vocab)\n",
    "neg_frequency_dict = generate_words_frequency(train_neg, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "looking-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of unique word in the vocab\n",
    "V = len(vocab)\n",
    "\n",
    "# calculate N_class for both positive and negative classes\n",
    "N_pos = np.sum([v for k, v in pos_frequency_dict.items()])\n",
    "N_neg = np.sum([v for k, v in neg_frequency_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "polish-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laplacian smoothing\n",
    "prob_w_given_pos_dict = {k: (v + 1)/(N_pos + V) for k, v in pos_frequency_dict.items()}\n",
    "prob_w_given_neg_dict = {k: (v + 1)/(N_pos + V) for k, v in neg_frequency_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "stock-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate lambda\n",
    "lambda_dict = {k: np.log(v/prob_w_given_neg_dict.get(k)) for k, v in prob_w_given_pos_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "advisory-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_prior is log(#positive_tweets / #negative_tweets)\n",
    "log_prior = np.log(len(train_pos)/len(train_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "complete-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg))).reshape(len(test_pos)+len(test_neg), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fiscal-greek",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.55"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_performence(test_x, test_y, lambda_dict, log_prior):\n",
    "    preds = np.zeros((len(test_x), 1))\n",
    "    for idx, tweet in enumerate(test_x):\n",
    "        tweet_tokens = preprocess_tweet(tweet)\n",
    "        res = log_prior\n",
    "        for word in tweet_tokens:\n",
    "            if word in lambda_dict:\n",
    "                res += lambda_dict[word]\n",
    "        \n",
    "        if res > 0:\n",
    "            preds[idx, 0] = 1\n",
    "        \n",
    "            \n",
    "    correct = np.sum(test_y == preds)\n",
    "    \n",
    "    return correct * 100/len(test_x)\n",
    "            \n",
    "evaluate_performence(test_pos + test_neg, test_y, lambda_dict, log_prior)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "social-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tweet, lambda_dict, log_prior):\n",
    "    tweet_tokens = preprocess_tweet(tweet)\n",
    "    res = log_prior\n",
    "    for word in tweet_tokens:\n",
    "        if word in lambda_dict:\n",
    "            res += lambda_dict[word]\n",
    "                \n",
    "    if res > 0:\n",
    "        print(\"Positive Sentiment\")\n",
    "    else:\n",
    "        print(\"Negative Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "continued-charlotte",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Sentiment\n"
     ]
    }
   ],
   "source": [
    "predict(\"I am happy\", lambda_dict, log_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "sexual-essay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Sentiment\n"
     ]
    }
   ],
   "source": [
    "predict(\"this movie should have been great\", lambda_dict, log_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "demanding-pharmacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Sentiment\n"
     ]
    }
   ],
   "source": [
    "predict(\"this movie is really bad\", lambda_dict, log_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-setup",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
